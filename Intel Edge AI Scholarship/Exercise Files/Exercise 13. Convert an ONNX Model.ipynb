{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert an ONNX Model\n",
    "\n",
    "Make sure to click the button below before you get started to source the correct environment.\n",
    "\n",
    "<button id=\"ulab-button-0bd71d51\" class=\"ulab-btn--primary\"></button>\n",
    "\n",
    "### Exercise Instructions\n",
    "\n",
    "In this exercise, you'll convert an ONNX Model into an Intermediate Representation using the \n",
    "Model Optimizer. You can find the related documentation [here](https://docs.openvinotoolkit.org/2018_R5/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_ONNX.html).\n",
    "\n",
    "For this exercise, first download the bvlc_alexnet model from [here](https://s3.amazonaws.com/download.onnx/models/opset_8/bvlc_alexnet.tar.gz). Use the `tar -xvf` command with the downloaded file to unpack it.\n",
    "\n",
    "Follow the documentation above and feed in the ONNX model to the Model Optimizer.\n",
    "\n",
    "If the conversion is successful, the terminal should let you know that it generated an IR model.\n",
    "The locations of the `.xml` and `.bin` files, as well as execution time of the Model Optimizer,\n",
    "will also be output.\n",
    "\n",
    "### PyTorch models\n",
    "\n",
    "Note that we will only cover converting directly from an ONNX model here. If you are interested\n",
    "in converting a PyTorch model using ONNX for use with OpenVINO, check out this [link](https://michhar.github.io/convert-pytorch-onnx/) for the steps to do so. From there, you can follow the steps in the rest\n",
    "of this exercise once you have an ONNX model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution Convert an ONNX Model\n",
    "\n",
    "First, you can start by checking out the documentation specific to ONNX models [here](https://docs.openvinotoolkit.org/2018_R5/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_ONNX.html).\n",
    "\n",
    "Now, given that I was in the directory with the ONNX model file, here was the \n",
    "full path to convert my model:\n",
    "\n",
    "```\n",
    "python /opt/intel/openvino/deployment_tools/model_optimizer/mo.py --input_model model.onnx\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
