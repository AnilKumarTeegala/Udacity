{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrate the Inference Engine in An Edge App\n",
    "\n",
    "Make sure to click the button below before you get started to source the correct environment.\n",
    "\n",
    "<button id=\"ulab-button-d44d77ce\" class=\"ulab-btn--primary\"></button>\n",
    "\n",
    "You've come a long way from the first lesson where most of the code for working with\n",
    "the OpenVINO toolkit was happening in the background. You worked with pre-trained models,\n",
    "moved up to converting any trained model to an Intermediate Representation with the\n",
    "Model Optimizer, and even got the model loaded into the Inference Engine and began making\n",
    "inference requests.\n",
    "\n",
    "In this final exercise of this lesson, you'll close off the OpenVINO workflow by extracting\n",
    "the results of the inference request, and then integrating the Inference Engine into an existing\n",
    "application. You'll still be given some of the overall application infrastructure, as more that of\n",
    "will come in the next lesson, but all of that is outside of OpenVINO itself.\n",
    "\n",
    "You will also add code allowing you to try out various confidence thresholds with the model,\n",
    "as well as changing the visual look of the output, like bounding box colors.\n",
    "\n",
    "Now, it's up to you which exact model you want to use here, although you are able to just\n",
    "re-use the model you converted with TensorFlow before for an easy bounding box dectector.\n",
    "\n",
    "Note that this application will run with a video instead of just images like we've done before.\n",
    "\n",
    "So, your tasks are to:\n",
    "\n",
    "1. Convert a bounding box model to an IR with the Model Optimizer.\n",
    "2. Pre-process the model as necessary.\n",
    "3. Use an async request to perform inference on each video frame.\n",
    "4. Extract the results from the inference request.\n",
    "5. Add code to make the requests and feed back the results within the application.\n",
    "6. Perform any necessary post-processing steps to get the bounding boxes.\n",
    "7. Add a command line argument to allow for different confidence thresholds for the model.\n",
    "8. Add a command line argument to allow for different bounding box colors for the output.\n",
    "9. Correctly utilize the command line arguments in #3 and #4 within the application.\n",
    "\n",
    "When you are done, feed your model to `app.py`, and it will generate `out.mp4`, which you\n",
    "can download and view. *Note that this app will take a little bit longer to run.* Also, if you need\n",
    "to re-run inference, delete the `out.mp4` file first.\n",
    "\n",
    "You only need to feed the model with `-m` before adding the customization; you should set\n",
    "defaults for any additional arguments you add for the color and confidence so that the user\n",
    "does not always need to specify them.\n",
    "\n",
    "```bash\n",
    "python app.py -m {your-model-path.xml}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrate the Inference Engine - Solution\n",
    "\n",
    "Let's step through the tasks one by one, with a potential approach for each.\n",
    "\n",
    "> Convert a bounding box model to an IR with the Model Optimizer.\n",
    "\n",
    "I used the SSD Mobilenet V2 architecture from TensorFlow from the earlier lesson here. Note\n",
    "that the original was downloaded in a separate workspace, so I needed to download it again\n",
    "and then convert it.\n",
    "\n",
    "```\n",
    "python /opt/intel/openvino/deployment_tools/model_optimizer/mo.py --input_model frozen_inference_graph.pb --tensorflow_object_detection_api_pipeline_config pipeline.config --reverse_input_channels --tensorflow_use_custom_operations_config /opt/intel/openvino/deployment_tools/model_optimizer/extensions/front/tf/ssd_v2_support.json\n",
    "```\n",
    "\n",
    "> Extract the results from the inference request\n",
    "\n",
    "```\n",
    "self.exec_network.requests[0].outputs[self.output_blob]\n",
    "```\n",
    "\n",
    "> Add code to make the requests and feed back the results within the application\n",
    "\n",
    "```\n",
    "self.exec_network.start_async(request_id=0, inputs={self.input_blob: image})\n",
    "...\n",
    "status = self.exec_network.requests[0].wait(-1)\n",
    "```\n",
    "\n",
    "> Add a command line argument to allow for different confidence thresholds for the model\n",
    "\n",
    "I chose to use `-ct` as the argument name here, and added it to the existing arguments.\n",
    "\n",
    "```\n",
    "optional.add_argument(\"-ct\", help=\"The confidence threshold to use with the bounding boxes\", default=0.5)\n",
    "```\n",
    "\n",
    "I set a default of 0.5, so it does not need to be input by the user every time. \n",
    "\n",
    "> Add a command line argument to allow for different bounding box colors for the output\n",
    "\n",
    "Similarly, I added the `-c` argument for inputting a bounding box color.\n",
    "Note that in my approach, I chose to only allow \"RED\", \"GREEN\" and \"BLUE\", which also\n",
    "impacts what I'll do in the next step; there are many possible approaches here.\n",
    "\n",
    "```\n",
    "optional.add_argument(\"-c\", help=\"The color of the bounding boxes to draw; RED, GREEN or BLUE\", default='BLUE')\n",
    "```\n",
    "\n",
    "> Correctly utilize the command line arguments in #3 and #4 within the application\n",
    "\n",
    "Both of these will come into play within the `draw_boxes` function. For the first, a new line\n",
    "should be added before extracting the bounding box points that check whether `box[2]`\n",
    "(e.g. the probability of a given box) is above `args.ct` - assuming you have added \n",
    "`args.ct` as an argument passed to the `draw_boxes` function. If not, the box\n",
    "should not be drawn. Without this, any random box will be drawn, which could be a ton of\n",
    "very unlikely bounding box detections.\n",
    "\n",
    "The second is just a small adjustment to the `cv2.rectangle` function that draws the \n",
    "bounding boxes we found to be above `args.ct`. I actually added a function to match\n",
    "the different potential colors up to their RGB values first, due to how I took them in from the\n",
    "command line:\n",
    "\n",
    "```\n",
    "def convert_color(color_string):\n",
    "    '''\n",
    "    Get the BGR value of the desired bounding box color.\n",
    "    Defaults to Blue if an invalid color is given.\n",
    "    '''\n",
    "    colors = {\"BLUE\": (255,0,0), \"GREEN\": (0,255,0), \"RED\": (0,0,255)}\n",
    "    out_color = colors.get(color_string)\n",
    "    if out_color:\n",
    "        return out_color\n",
    "    else:\n",
    "        return colors['BLUE']\n",
    "```\n",
    "\n",
    "I can also add the tuple returned from this function as an additional `color` argument to feed to\n",
    "`draw_boxes`.\n",
    "\n",
    "Then, the line where the bounding boxes are drawn becomes:\n",
    "\n",
    "```\n",
    "cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), color, 1)\n",
    "```\n",
    "\n",
    "I was able to run my app, if I was using the converted TF model from earlier (and placed in the \n",
    "current directory), using the below:\n",
    "\n",
    "```bash\n",
    "python app.py -m frozen_inference_graph.xml\n",
    "```\n",
    "\n",
    "Or, if I added additional customization with a confidence threshold of 0.6 and blue boxes:\n",
    "\n",
    "```bash\n",
    "python app.py -m frozen_inference_graph.xml -ct 0.6 -c BLUE\n",
    "```\n",
    "\n",
    "[Note that I placed my customized app actually in `app-custom.py`]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
