{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed an IR to the Inference Engine\n",
    "\n",
    "\n",
    "Make sure to click the button below before you get started to source the correct environment.\n",
    "\n",
    "<button id=\"ulab-button-6f2a60e5\" class=\"ulab-btn--primary\"></button>\n",
    "\n",
    "Earlier in the course, you were focused on working with the Intermediate Representation (IR)\n",
    "models themselves, while mostly glossing over the use of the actual Inference Engine with\n",
    "the model.\n",
    "\n",
    "Here, you'll import the Python wrapper for the Inference Engine (IE), and practice using \n",
    "different IRs with it. You will first add each IR as an `IENetwork`, and check whether the layers \n",
    "of that network are supported by the classroom CPU.\n",
    "\n",
    "Since the classroom workspace is using an Intel CPU, you will also need to add a CPU\n",
    "extension to the `IECore`.\n",
    "\n",
    "Once you have verified all layers are supported (when the CPU extension is added),\n",
    "you will load the given model into the Inference Engine.\n",
    "\n",
    "Note that the `.xml` file of the IR should be given as an argument when running the script.\n",
    "\n",
    "To test your implementation, you should be able to successfully load each of the three IR\n",
    "model files we have been working with throughout the course so far, which you can find in the\n",
    "`/home/workspace/models` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution: Feed an IR to the Inference Engine\n",
    "\n",
    "To start, I'll import the `IENetwork` and `IECore` from the Inference Engine. The first of\n",
    "these is what will hold the Intermediate Representation object, while the second is the \n",
    "Python Plugin for working with the Inference Engine.\n",
    "\n",
    "```\n",
    "from openvino.inference_engine import IENetwork, IECore\n",
    "```\n",
    "\n",
    "I will go ahead and initialize a `plugin` variable with the `IECore` object now as well.\n",
    "\n",
    "```\n",
    "plugin = IECore()\n",
    "```\n",
    "\n",
    "Now, I can load the separate IR models into an `IENetwork` object, which I'll call `net`.\n",
    "\n",
    "```\n",
    "net = IENetwork(model=model_xml, weights=model_bin)\n",
    "```\n",
    "\n",
    "As discussed before, the `.xml` file of the IR is the model architecture, while the `.bin` file\n",
    "contains weights and biases. `model_xml` and `model_bin` should be the paths to these\n",
    "files.\n",
    "\n",
    "Before loading this `net` into the `plugin`, we need to check whether all of the layers are\n",
    "supported by the `plugin`. We can get the plugin's supported layers of the IR by using the\n",
    "`.query_network()` function of an [`IECore`](https://docs.openvinotoolkit.org/latest/classie__api_1_1IECore.html):\n",
    "\n",
    "```\n",
    "supported_layers = plugin.query_network(network=net, device_name=\"CPU\")\n",
    "```\n",
    "\n",
    "Note that the `device` argument here can also be `\"GPU\"`, `\"FPGA\"` or `\"MYRIAD\"`, depending\n",
    "on what hardware is being used. `\"MYRIAD\"` is used for the Intel Neural Compute Stick.\n",
    "\n",
    "Then, we can iterate through the layers in the `net` itself, to gather any unsupported layers.\n",
    "\n",
    "```\n",
    "unsupported_layers = [l for l in net.layers.keys() if l not in supported_layers]\n",
    "```\n",
    "\n",
    "If the length of the `unsupported_layers` list is not zero, the model is not going to be able\n",
    "to run on this device with the Inference Engine. So, we should add a `print` statement \n",
    "or `log` to the console that an extension may be necessary, and then exit the program.\n",
    "\n",
    "In our case, we will be able to run the included IRs by adding a CPU extension. For Linux\n",
    "machines, like the classroom workspace, these are usually found in the following directory:\n",
    "```\n",
    "<OpenVINO install dir>/deployment_tools/inference_engine/lib/intel64\n",
    "```\n",
    "\n",
    "I hard-coded the location of the relevant CPU extension into the starter code. In this case,\n",
    "it is the `libcpu_extension_sse4.so` CPU extension that you'll want to add.\n",
    "\n",
    "Back in the `IECore` [documentation](https://docs.openvinotoolkit.org/latest/classie__api_1_1IECore.html),\n",
    "we see there is an `.add_extension` function, which just takes the path to the CPU\n",
    "extension and device name:\n",
    "\n",
    "```\n",
    "plugin.add_extension(cpu_extension, “CPU”)\n",
    "```\n",
    "\n",
    "Now, if we check for unsupported layers again, there should be zero that are unsupported.\n",
    "\n",
    "Finally, we can  `.load_network` function from `IECore` to load the model into the Inference Engine:\n",
    "\n",
    "```\n",
    "plugin.load_network(net, “CPU”)\n",
    "```\n",
    "\n",
    "To make sure your implementation works appropriately, you should use each of the three\n",
    "models in the workspace with `feed_network.py`, such as:\n",
    "\n",
    "```bash\n",
    "python feed_network.py -m /home/workspace/models/human-pose-estimation-0001.xml\n",
    "```\n",
    "\n",
    "Now, we're ready to start making inference requests, which we'll look at next.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
