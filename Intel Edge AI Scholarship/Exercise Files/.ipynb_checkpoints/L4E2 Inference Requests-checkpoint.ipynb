{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Requests\n",
    "\n",
    "Make sure to click the button below before you get started to source the correct environment.\n",
    "\n",
    "<button id=\"ulab-button-ceb2f99a\" class=\"ulab-btn--primary\"></button>\n",
    "\n",
    "In the previous exercise, you loaded Intermediate Representations (IRs) into the Inference\n",
    "Engine. Now that we've covered some of the topics around requests, including the difference\n",
    "between synchronous and asynchronous requests, you'll add additional code to make\n",
    "inference requests to the Inference Engine.\n",
    "\n",
    "Given an `ExecutableNetwork` that is the IR loaded into the Inference Engine, your task is to:\n",
    "\n",
    "1. Perform a synchronous request\n",
    "2. Start an asynchronous request given an input image frame\n",
    "3. Wait for the asynchronous request to complete\n",
    "\n",
    "Note that we'll cover handling the results of the request shortly, so you don't need to worry\n",
    "about that just yet. This will get you practice with both types of requests with the Inference\n",
    "Engine.\n",
    "\n",
    "You will perform the above tasks within `inference.py`. This will take three arguments,\n",
    "one for the model, one for the test image, and the last for what type of inference request\n",
    "should be made.\n",
    "\n",
    "You can use `test.py` afterward to verify your code successfully makes inference requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution Inference Requests\n",
    "\n",
    "\n",
    "To get started, let's check out the [documentation](https://docs.openvinotoolkit.org/latest/classie__api_1_1ExecutableNetwork.html) for `ExecutableNetwork`.\n",
    "\n",
    "I noticed two functions that seem relevant to the task at hand: `infer` and `start_async`.\n",
    "\n",
    "While the second function pretty clearly refers to an asynchronous request based on the name, \n",
    "we can see from the notes that `infer` is used for synchronous requests. In fact, it looks like \n",
    "that function will directly return our results.\n",
    "\n",
    "### Synchronous Request\n",
    "\n",
    "So, if we have an `ExecutableNetwork` called `exec_net`, to make a synchronous request, \n",
    "we can do this:\n",
    "\n",
    "```\n",
    "result = exec_net.infer({'data': frame})\n",
    "```\n",
    "\n",
    "So that one is pretty quick and easy; you just need to feed the image frame in and the\n",
    "synchronous request is made, returning the image.\n",
    "\n",
    "### Asynchronous Request\n",
    "\n",
    "The asynchronous request is a two-parter. First, you use the `start_async` function, which is\n",
    "going to return a `InferRequest` object. \n",
    "\n",
    "```\n",
    "exec_net.start_async(request_id=request_id, inputs={input_blob: frame})\n",
    "```\n",
    "\n",
    "The `input_blob` here is just the input layer of the network (`next(iter(net.inputs))`).\n",
    "\n",
    "Since it's asynchronous, your device could be used to go ahead and capture the next frame \n",
    "of input while the current one is having inference performed on it. Therefore, to continue \n",
    "processing with the current frame, the application will need to `wait` until the current inference \n",
    "request is finished.\n",
    "\n",
    "You can find the [documentation](https://docs.openvinotoolkit.org/latest/classie__api_1_1InferRequest.html) for `wait`\n",
    "within `InferRequest`'s documentation. Back in `ExecutableNetwork`, we can see it has\n",
    "a class attribute of `requests`, where you can work with the current tuple of `InferRequests`.\n",
    "\n",
    "To use `wait`, we want to feed in a `-1` as an argument, as that will make the process wait\n",
    "until the inference results are available. Otherwise, we might try to extract and handle the\n",
    "model's outputs, with nothing to use.\n",
    "\n",
    "```\n",
    "status = exec_net.requests[request_id].wait(-1)\n",
    "```\n",
    "\n",
    "We can use a `request_id` of `0` here as we don't have any other requests, but if you had\n",
    "another request to make, you'd use a `1` for `request_id`. \n",
    "\n",
    "This will return a status code - if it's `0`, the inference request is complete, and the results\n",
    "can now be extracted. Let's look at that next.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
