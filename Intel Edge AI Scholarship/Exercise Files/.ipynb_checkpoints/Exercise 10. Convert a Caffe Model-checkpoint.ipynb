{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert a Caffe Model\n",
    "\n",
    "Make sure to click the button below before you get started to source the correct environment.\n",
    "\n",
    "<button id=\"ulab-button-d0a57724\" class=\"ulab-btn--primary\"></button>\n",
    "\n",
    "In this exercise, you'll convert a Caffe Model into an Intermediate Representation using the \n",
    "Model Optimizer. You can find the related documentation [here](https://docs.openvinotoolkit.org/2018_R5/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_Caffe.html).\n",
    "\n",
    "For this exercise, first download the SqueezeNet V1.1 model by cloning [this repository](https://github.com/DeepScale/SqueezeNet). \n",
    "\n",
    "Follow the documentation above and feed in the Caffe model to the Model Optimizer.\n",
    "\n",
    "If the conversion is successful, the terminal should let you know that it generated an IR model.\n",
    "The locations of the `.xml` and `.bin` files, as well as execution time of the Model Optimizer,\n",
    "will also be output.\n",
    "\n",
    "### Hints & Troubleshooting\n",
    "\n",
    "You will need to specify `--input_proto` if the `.prototxt` file is not named the same as the model.\n",
    "\n",
    "There is an important note in the documentation after the section **Supported Topologies** \n",
    "regarding Caffe models trained on ImageNet. If you notice poor performance in inference, you\n",
    "may need to specify mean and scale values in your arguments.\n",
    "\n",
    "```\n",
    "python /opt/intel/openvino/deployment_tools/model_optimizer/mo.py --input_model squeezenet_v1.1.caffemodel --input_proto deploy.prototxt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution Convert a Caffe Model - Solution\n",
    "\n",
    "First, you can start by checking out the documentation specific to Caffe models [here](https://docs.openvinotoolkit.org/2018_R5/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_Caffe.html).\n",
    "\n",
    "I did notice an additional helpful argument here: `--input_proto`, which is used to specify\n",
    "a `.prototxt` file to pair with the `.caffemodel` file when the model name and `.prototxt`\n",
    "filename do not match.\n",
    "\n",
    "Now, given that I was in the directory with the Caffe model file & `.prototxt` file, here was the \n",
    "full path to convert my model:\n",
    "\n",
    "```\n",
    "python /opt/intel/openvino/deployment_tools/model_optimizer/mo.py --input_model squeezenet_v1.1.caffemodel --input_proto deploy.prototxt\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
